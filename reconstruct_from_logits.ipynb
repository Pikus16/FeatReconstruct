{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/content_understanding/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m std\u001b[39m=\u001b[39m(\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m test_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         [\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             transforms\u001b[39m.\u001b[39mResize(resize_size, interpolation\u001b[39m=\u001b[39mInterpolationMode\u001b[39m.\u001b[39mBILINEAR),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_set \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtest_transform)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mlen\u001b[39m(test_set)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    304\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m ):\n\u001b[0;32m--> 310\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    311\u001b[0m         root,\n\u001b[1;32m    312\u001b[0m         loader,\n\u001b[1;32m    313\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    314\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    315\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    316\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 145\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    146\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    193\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/'"
     ]
    }
   ],
   "source": [
    "resize_size = 256\n",
    "crop_size = 224\n",
    "mean=(0.485, 0.456, 0.406)\n",
    "std=(0.229, 0.224, 0.225)\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(resize_size, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "test_set = datasets.ImageFolder(f'/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/', transform=test_transform)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 50000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=(0.485, 0.456, 0.406)\n",
    "crop_size = 224\n",
    "std=(0.229, 0.224, 0.225)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((crop_size, crop_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "test_set = datasets.CIFAR10(root=f'/home/ubuntu/data/cifar10', train=False, download=True, transform=test_transform)\n",
    "train_set = datasets.CIFAR10(root=f'/home/ubuntu/data/cifar10', train=True, download=True, transform=test_transform)\n",
    "len(test_set), len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_np(original, to_append):\n",
    "    if original is None:\n",
    "        return to_append\n",
    "    else:\n",
    "        return np.concatenate((original, to_append))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, dataset, batch_size=32, num_workers=8):\n",
    "    original_fc = copy.deepcopy(model.fc)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size, num_workers=num_workers,shuffle=False)\n",
    "    all_feats = None\n",
    "    for (images, label) in tqdm(dataloader, total=len(dataset) // batch_size):\n",
    "        with torch.no_grad():\n",
    "            output = model(images.to(device)).cpu().numpy()\n",
    "            all_feats = append_np(all_feats, output)\n",
    "    model.fc = original_fc\n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [01:15, 20.62it/s]                          \n",
      "313it [00:07, 39.46it/s]                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 512), (10000, 512))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_feats_train = get_features(model, train_set)\n",
    "cifar_feats_test = get_features(model, test_set)\n",
    "cifar_feats_train.shape, cifar_feats_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(model, dataset, batch_size=32, num_workers=8):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size, num_workers=num_workers,shuffle=False)\n",
    "    all_logits = None\n",
    "    all_labels = None\n",
    "    for (images, label) in tqdm(dataloader, total=len(dataset) // batch_size):\n",
    "        with torch.no_grad():\n",
    "            output = model(images.to(device)).cpu().numpy()\n",
    "            all_logits = append_np(all_logits, output)\n",
    "            all_labels = append_np(all_labels, label.numpy())\n",
    "    return all_logits, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [02:00, 13.02it/s]                          \n",
      "313it [00:08, 35.56it/s]                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 1000), (10000, 1000))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_logits_train, cifar_labels_train = get_logits(model, train_set)\n",
    "cifar_logits_test, cifar_labels_test = get_logits(model, test_set)\n",
    "cifar_logits_train.shape, cifar_logits_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reconstruct(logits, features, test_logits, test_features):\n",
    "    lin = torch.nn.Linear(logits.shape[1], features.shape[1])\n",
    "    rec_loss = torch.nn.MSELoss()\n",
    "    epochs = 100\n",
    "    batch_size = 16\n",
    "    optimizer = torch.optim.SGD(lin.parameters(), lr = 0.1, momentum = 0.9)\n",
    "    \n",
    "    logits = torch.FloatTensor(logits)\n",
    "    features = torch.FloatTensor(features)\n",
    "    test_logits = torch.FloatTensor(test_logits)\n",
    "    test_features = torch.FloatTensor(test_features)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        for b in range(0, len(logits), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = logits[b:b+batch_size]\n",
    "            y = features[b:b+batch_size]\n",
    "            pred_y = lin(x)\n",
    "            loss = rec_loss(pred_y, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            lin.eval()\n",
    "            test_loss = 0\n",
    "            for b in range(0, len(test_logits), batch_size):\n",
    "                with torch.no_grad():\n",
    "                    x = test_logits[b:b+batch_size]\n",
    "                    y = test_features[b:b+batch_size]\n",
    "                    pred_y = lin(x)\n",
    "                    loss = rec_loss(pred_y, y)\n",
    "                    test_loss += loss.item()\n",
    "            print(f\"Epoch {i}, train loss={total_loss}, test loss = {test_loss}\")\n",
    "            lin.train()\n",
    "            \n",
    "    return lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss=71.39638428250328, test loss = 5.738541886210442\n",
      "Epoch 10, train loss=6.093450883287005, test loss = 1.5566608069930226\n",
      "Epoch 20, train loss=3.9516613416490145, test loss = 1.0289810613030568\n",
      "Epoch 30, train loss=2.9362547773635015, test loss = 0.7705695214681327\n",
      "Epoch 40, train loss=2.318786834453931, test loss = 0.6127101713209413\n",
      "Epoch 50, train loss=1.897863550373586, test loss = 0.5052875905530527\n",
      "Epoch 60, train loss=1.5909859931562096, test loss = 0.42697102518286556\n",
      "Epoch 70, train loss=1.3570018215395976, test loss = 0.36705848306883126\n",
      "Epoch 80, train loss=1.1727529991185293, test loss = 0.31959150033071637\n",
      "Epoch 90, train loss=1.024091051222058, test loss = 0.2809903149609454\n"
     ]
    }
   ],
   "source": [
    "recon_model = learn_reconstruct(cifar_logits_train, cifar_feats_train, cifar_logits_test, cifar_feats_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feat_recon(logits, recon_model_):\n",
    "    batch_size = 16\n",
    "    logits = torch.FloatTensor(logits)\n",
    "\n",
    "    all_feats = None\n",
    "    with torch.no_grad():\n",
    "        for b in range(0, len(logits), batch_size):\n",
    "            x = logits[b:b+batch_size]\n",
    "            pred_y = recon_model_(x).numpy()\n",
    "            all_feats = append_np(all_feats, pred_y)\n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 512)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_cifar_feats_test = do_feat_recon(cifar_logits_test, recon_model)\n",
    "recon_cifar_feats_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(feats, lin):\n",
    "    lin.eval()\n",
    "    batch_size=16\n",
    "    all_preds = None\n",
    "    feats = torch.FloatTensor(feats)\n",
    "    for b in range(0, len(feats), batch_size):\n",
    "        with torch.no_grad():\n",
    "            x = feats[b:b+batch_size]\n",
    "            pred = lin(x).numpy()\n",
    "            all_preds = append_np(all_preds, pred)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1000), (10000, 1000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_orig = get_preds(cifar_feats_test, model.fc.cpu())\n",
    "preds_recon = get_preds(recon_cifar_feats_test,  model.fc.cpu())\n",
    "preds_orig.shape, preds_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9918"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_recon = np.argmax(preds_recon, axis=1)\n",
    "top_orig = np.argmax(preds_orig, axis=1)\n",
    "np.mean(top_orig == top_recon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a resnet18, pretrained on ImageNet, learns the features from the logits with a single linear layer. \n",
    "Training on CIFAR-100 train, and testing on CIFAR-100 test, we get a reconstruction, MSE loss of 0.28 total over the 10k test examples.\n",
    "If we take these reconstructed features and then use the model's linear head, the top-1 prediction matches the original prediction 99.18% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content_understanding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d36b139402f0f8909133622e5e80cdd43397350f551386f6df555aa508ab69d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
