{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/content_understanding/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, models, datasets\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the predicted logits from a ResNet18, pretrained on ImageNet, can we learn the features with a single linear layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m std\u001b[39m=\u001b[39m(\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m test_transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         [\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m             transforms\u001b[39m.\u001b[39mResize(resize_size, interpolation\u001b[39m=\u001b[39mInterpolationMode\u001b[39m.\u001b[39mBILINEAR),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m test_set \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mImageFolder(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtest_transform)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/reconstruct_from_logits/reconstruct_from_logits.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mlen\u001b[39m(test_set)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:310\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    304\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m ):\n\u001b[0;32m--> 310\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    311\u001b[0m         root,\n\u001b[1;32m    312\u001b[0m         loader,\n\u001b[1;32m    313\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;49;00m is_valid_file \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    314\u001b[0m         transform\u001b[39m=\u001b[39;49mtransform,\n\u001b[1;32m    315\u001b[0m         target_transform\u001b[39m=\u001b[39;49mtarget_transform,\n\u001b[1;32m    316\u001b[0m         is_valid_file\u001b[39m=\u001b[39;49mis_valid_file,\n\u001b[1;32m    317\u001b[0m     )\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 145\u001b[0m     classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_classes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot)\n\u001b[1;32m    146\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:219\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(\u001b[39mself\u001b[39m, directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m    193\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m find_classes(directory)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_classes\u001b[39m(directory: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[List[\u001b[39mstr\u001b[39m], Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[39m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(entry\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(directory) \u001b[39mif\u001b[39;00m entry\u001b[39m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any class folder in \u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/'"
     ]
    }
   ],
   "source": [
    "resize_size = 256\n",
    "crop_size = 224\n",
    "mean=(0.485, 0.456, 0.406)\n",
    "std=(0.229, 0.224, 0.225)\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(resize_size, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "test_set = datasets.ImageFolder(f'/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/val/', transform=test_transform)\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 50000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=(0.485, 0.456, 0.406)\n",
    "crop_size = 224\n",
    "std=(0.229, 0.224, 0.225)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((crop_size, crop_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "test_set = datasets.CIFAR10(root=f'/home/ubuntu/data/cifar10', train=False, download=True, transform=test_transform)\n",
    "train_set = datasets.CIFAR10(root=f'/home/ubuntu/data/cifar10', train=True, download=True, transform=test_transform)\n",
    "len(test_set), len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_np(original, to_append):\n",
    "    if original is None:\n",
    "        return to_append\n",
    "    else:\n",
    "        return np.concatenate((original, to_append))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, dataset, batch_size=32, num_workers=8):\n",
    "    original_fc = copy.deepcopy(model.fc)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size, num_workers=num_workers,shuffle=False)\n",
    "    all_feats = None\n",
    "    for (images, label) in tqdm(dataloader, total=len(dataset) // batch_size):\n",
    "        with torch.no_grad():\n",
    "            output = model(images.to(device)).cpu().numpy()\n",
    "            all_feats = append_np(all_feats, output)\n",
    "    model.fc = original_fc\n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [01:15, 20.81it/s]                          \n",
      "313it [00:08, 39.12it/s]                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 512), (10000, 512))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_feats_train = get_features(model, train_set)\n",
    "cifar_feats_test = get_features(model, test_set)\n",
    "cifar_feats_train.shape, cifar_feats_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def get_logits(model, dataset, batch_size=32, num_workers=8):\\n    model.to(device)\\n    model.eval()\\n    dataloader = DataLoader(dataset,batch_size=batch_size, num_workers=num_workers,shuffle=False)\\n    all_logits = None\\n    all_labels = None\\n    for (images, label) in tqdm(dataloader, total=len(dataset) // batch_size):\\n        with torch.no_grad():\\n            output = model(images.to(device)).cpu().numpy()\\n            all_logits = append_np(all_logits, output)\\n            all_labels = append_np(all_labels, label.numpy())\\n    return all_logits, all_labels'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_logits(model, dataset, batch_size=32, num_workers=8):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size, num_workers=num_workers,shuffle=False)\n",
    "    all_logits = None\n",
    "    all_labels = None\n",
    "    for (images, label) in tqdm(dataloader, total=len(dataset) // batch_size):\n",
    "        with torch.no_grad():\n",
    "            output = model(images.to(device)).cpu().numpy()\n",
    "            all_logits = append_np(all_logits, output)\n",
    "            all_labels = append_np(all_labels, label.numpy())\n",
    "    return all_logits, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat = self.features[idx]\n",
    "        if self.labels is None:\n",
    "            return feat\n",
    "        else:\n",
    "            return feat, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(feats, lin):\n",
    "    lin.eval()\n",
    "    batch_size=16\n",
    "    num_workers=8\n",
    "\n",
    "    all_preds = None\n",
    "    dataloader = DataLoader(FeatDataset(feats), batch_size=batch_size, num_workers=num_workers,shuffle=False)\n",
    "    for batch_feats in tqdm(dataloader, total=len(feats) // batch_size):\n",
    "        with torch.no_grad():\n",
    "            pred = lin(batch_feats).numpy()\n",
    "            all_preds = append_np(all_preds, pred)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [02:56<00:00, 17.66it/s]\n",
      "100%|██████████| 625/625 [00:05<00:00, 112.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 1000), (10000, 1000))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_logits_train = get_logits(cifar_feats_train, model.fc.cpu())#get_logits(model, train_set)\n",
    "cifar_logits_test = get_logits(cifar_feats_test, model.fc.cpu())#\n",
    "cifar_logits_train.shape, cifar_logits_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recon_loss(recon_model, logits, features, batch_size=16):\n",
    "    rec_loss = torch.nn.MSELoss()\n",
    "    recon_model.eval()\n",
    "    test_loss = 0\n",
    "    for b in range(0, len(logits), batch_size):\n",
    "        with torch.no_grad():\n",
    "            x = logits[b:b+batch_size]\n",
    "            y = features[b:b+batch_size]\n",
    "            pred_y = recon_model(x)\n",
    "            loss = rec_loss(pred_y, y)\n",
    "            test_loss += loss.item()\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reconstruct(logits, features, test_logits, test_features):\n",
    "    lin = torch.nn.Linear(logits.shape[1], features.shape[1])\n",
    "    rec_loss = torch.nn.MSELoss()\n",
    "    epochs = 100\n",
    "    batch_size = 16\n",
    "    optimizer = torch.optim.SGD(lin.parameters(), lr = 0.1, momentum = 0.9)\n",
    "    \n",
    "    logits = torch.FloatTensor(logits)\n",
    "    features = torch.FloatTensor(features)\n",
    "    test_logits = torch.FloatTensor(test_logits)\n",
    "    test_features = torch.FloatTensor(test_features)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        for b in range(0, len(logits), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            x = logits[b:b+batch_size]\n",
    "            y = features[b:b+batch_size]\n",
    "            pred_y = lin(x)\n",
    "            loss = rec_loss(pred_y, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            test_loss = get_recon_loss(lin, test_logits, test_features, batch_size=batch_size)\n",
    "            print(f\"Epoch {i}, train loss={total_loss}, test loss = {test_loss}\")\n",
    "            lin.train()\n",
    "            \n",
    "    return lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss=71.42700271867216, test loss = 6.059823074378073\n",
      "Epoch 10, train loss=6.075193635420874, test loss = 1.572297875303775\n",
      "Epoch 20, train loss=3.936132945585996, test loss = 1.029624812770635\n",
      "Epoch 30, train loss=2.9237039703293703, test loss = 0.7714758660295047\n",
      "Epoch 40, train loss=2.3084190539084375, test loss = 0.6153458339977078\n",
      "Epoch 50, train loss=1.889220092096366, test loss = 0.5092021900927648\n",
      "Epoch 60, train loss=1.583752770384308, test loss = 0.4315956605132669\n",
      "Epoch 70, train loss=1.350930875953054, test loss = 0.3719757022045087\n",
      "Epoch 80, train loss=1.1676412418746622, test loss = 0.3245321188296657\n",
      "Epoch 90, train loss=1.0197711210639682, test loss = 0.285793171817204\n"
     ]
    }
   ],
   "source": [
    "recon_model = learn_reconstruct(cifar_logits_train, cifar_feats_train, cifar_logits_test, cifar_feats_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feat_recon(logits, recon_model_):\n",
    "    batch_size = 16\n",
    "    logits = torch.FloatTensor(logits)\n",
    "\n",
    "    all_feats = None\n",
    "    with torch.no_grad():\n",
    "        for b in range(0, len(logits), batch_size):\n",
    "            x = logits[b:b+batch_size]\n",
    "            pred_y = recon_model_(x).numpy()\n",
    "            all_feats = append_np(all_feats, pred_y)\n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 512)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_cifar_feats_test = do_feat_recon(cifar_logits_test, recon_model)\n",
    "recon_cifar_feats_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 112.99it/s]\n",
      "100%|██████████| 625/625 [00:05<00:00, 118.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000, 1000), (10000, 1000))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_orig = get_logits(cifar_feats_test, model.fc.cpu())\n",
    "preds_recon = get_logits(recon_cifar_feats_test,  model.fc.cpu())\n",
    "preds_orig.shape, preds_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9919"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_recon = np.argmax(preds_recon, axis=1)\n",
    "top_orig = np.argmax(preds_orig, axis=1)\n",
    "np.mean(top_orig == top_recon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on CIFAR-100 train, and testing on CIFAR-100 test, we get a reconstruction, MSE loss of 0.28 total over the 10k test examples.\n",
    "\n",
    "If we take these reconstructed features and then use the model's linear head, the top-1 prediction matches the original prediction 99.18% of the time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the predicted logits from a ResNet18, pretrained on ImageNet, can we learn the features from a pretrained ResNet50 with a single linear layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 212MB/s] \n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1563it [04:05,  6.36it/s]                          \n",
      "313it [00:19, 15.97it/s]                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 2048), (10000, 2048))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_feats_train_resnet50 = get_features(resnet50, train_set)\n",
    "cifar_feats_test_resnet50 = get_features(resnet50, test_set)\n",
    "cifar_feats_train_resnet50.shape, cifar_feats_test_resnet50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [06:02<00:00,  8.61it/s]\n",
      "100%|██████████| 625/625 [00:14<00:00, 42.94it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 2048), (10000, 2048))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_logits_train_resnet50  = get_logits(cifar_feats_train_resnet50 , resnet50.fc.cpu())#get_logits(model, train_set)\n",
    "cifar_logits_test_resnet50  = get_logits(cifar_feats_test_resnet50 , resnet50.fc.cpu())#\n",
    "cifar_logits_train_resnet50.shape, cifar_logits_test_resnet50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss=69.57651571463794, test loss = 7.393297683447599\n",
      "Epoch 10, train loss=28.515093367546797, test loss = 5.487259719520807\n",
      "Epoch 20, train loss=27.672723228111863, test loss = 5.346435167361051\n",
      "Epoch 30, train loss=27.309775521978736, test loss = 5.286164271645248\n",
      "Epoch 40, train loss=27.093121097888798, test loss = 5.250551599543542\n",
      "Epoch 50, train loss=26.94400716898963, test loss = 5.226170690264553\n",
      "Epoch 60, train loss=26.83283564262092, test loss = 5.208049410954118\n",
      "Epoch 70, train loss=26.74560745153576, test loss = 5.193862346466631\n",
      "Epoch 80, train loss=26.674702115822583, test loss = 5.182348498608917\n",
      "Epoch 90, train loss=26.61555330688134, test loss = 5.172754235565662\n"
     ]
    }
   ],
   "source": [
    "recon_model_resnet50 = learn_reconstruct(cifar_logits_train, cifar_feats_train_resnet50, cifar_logits_test, cifar_feats_test_resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:14<00:00, 44.03it/s] \n",
      "100%|██████████| 625/625 [00:14<00:00, 44.26it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9947"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_cifar_feats_test_resnet50 = do_feat_recon(cifar_logits_test, recon_model_resnet50)\n",
    "preds_orig_resnet50 = get_logits(cifar_feats_test_resnet50, resnet50.fc.cpu())\n",
    "preds_recon_resnet50 = get_logits(recon_cifar_feats_test_resnet50,  resnet50.fc.cpu())\n",
    "top_recon = np.argmax(preds_recon_resnet50, axis=1)\n",
    "top_orig = np.argmax(preds_orig_resnet50, axis=1)\n",
    "np.mean(top_orig == top_recon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on CIFAR-100 train, and testing on CIFAR-100 test, we get a reconstruction, MSE loss of 5.17 total over the 10k test examples.\n",
    "\n",
    "If we take these reconstructed features and then use the model's linear head, the top-1 prediction matches the original prediction 99.47% of the time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the predicted logits from a ResNet18, pretrained on ImageNet, can we learn the features with a single linear layer, while training / testing on a different dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://data.csail.mit.edu/places/places365/filelist_places365-standard.tar to /home/ubuntu/data/Places365_small/filelist_places365-standard.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67499008it [00:03, 19400614.23it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/data/Places365_small/filelist_places365-standard.tar to /home/ubuntu/data/Places365_small\n",
      "Downloading http://data.csail.mit.edu/places/places365/train_256_places365standard.tar to /home/ubuntu/data/Places365_small/train_256_places365standard.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26103685120/26103685120 [17:53<00:00, 24312527.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/data/Places365_small/train_256_places365standard.tar to /home/ubuntu/data/Places365_small\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/FeatReconstruct/reconstruct_from_logits.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbenp_main/home/ubuntu/code/FeatReconstruct/reconstruct_from_logits.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m datasets\u001b[39m.\u001b[39;49mPlaces365(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/ubuntu/data/Places365_small\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain-standard\u001b[39;49m\u001b[39m'\u001b[39;49m,small\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/places365.py:83\u001b[0m, in \u001b[0;36mPlaces365.__init__\u001b[0;34m(self, root, split, small, download, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_file_list(download)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_images()\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/places365.py:153\u001b[0m, in \u001b[0;36mPlaces365.download_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe directory \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_dir\u001b[39m}\u001b[39;00m\u001b[39m already exists. If you want to re-download or re-extract the images, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdelete the directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    152\u001b[0m file, md5 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IMAGES_META[(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmall)]\n\u001b[0;32m--> 153\u001b[0m download_and_extract_archive(urljoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_BASE_URL, file), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, md5\u001b[39m=\u001b[39;49mmd5)\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    156\u001b[0m     os\u001b[39m.\u001b[39mrename(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_dir\u001b[39m.\u001b[39mrsplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_dir)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/utils.py:434\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    432\u001b[0m archive \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    433\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting \u001b[39m\u001b[39m{\u001b[39;00marchive\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mextract_root\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m extract_archive(archive, extract_root, remove_finished)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/utils.py:409\u001b[0m, in \u001b[0;36mextract_archive\u001b[0;34m(from_path, to_path, remove_finished)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m# We don't need to check for a missing key here, since this was already done in _detect_file_type()\u001b[39;00m\n\u001b[1;32m    407\u001b[0m extractor \u001b[39m=\u001b[39m _ARCHIVE_EXTRACTORS[archive_type]\n\u001b[0;32m--> 409\u001b[0m extractor(from_path, to_path, compression)\n\u001b[1;32m    410\u001b[0m \u001b[39mif\u001b[39;00m remove_finished:\n\u001b[1;32m    411\u001b[0m     os\u001b[39m.\u001b[39mremove(from_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torchvision/datasets/utils.py:273\u001b[0m, in \u001b[0;36m_extract_tar\u001b[0;34m(from_path, to_path, compression)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_extract_tar\u001b[39m(from_path: \u001b[39mstr\u001b[39m, to_path: \u001b[39mstr\u001b[39m, compression: Optional[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[39mwith\u001b[39;00m tarfile\u001b[39m.\u001b[39mopen(from_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mr:\u001b[39m\u001b[39m{\u001b[39;00mcompression[\u001b[39m1\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m compression \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tar:\n\u001b[0;32m--> 273\u001b[0m         tar\u001b[39m.\u001b[39;49mextractall(to_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/tarfile.py:2028\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[0;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[1;32m   2026\u001b[0m         tarinfo\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m \u001b[39m0o700\u001b[39m\n\u001b[1;32m   2027\u001b[0m     \u001b[39m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[0;32m-> 2028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tarinfo, path, set_attrs\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tarinfo\u001b[39m.\u001b[39;49misdir(),\n\u001b[1;32m   2029\u001b[0m                  numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2031\u001b[0m \u001b[39m# Reverse sort directories.\u001b[39;00m\n\u001b[1;32m   2032\u001b[0m directories\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a: a\u001b[39m.\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/tarfile.py:2069\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[0;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2066\u001b[0m     tarinfo\u001b[39m.\u001b[39m_link_target \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, tarinfo\u001b[39m.\u001b[39mlinkname)\n\u001b[1;32m   2068\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2069\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(tarinfo, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path, tarinfo\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m   2070\u001b[0m                          set_attrs\u001b[39m=\u001b[39;49mset_attrs,\n\u001b[1;32m   2071\u001b[0m                          numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[1;32m   2072\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2073\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrorlevel \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/tarfile.py:2141\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[0;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbg(\u001b[39m1\u001b[39m, tarinfo\u001b[39m.\u001b[39mname)\n\u001b[1;32m   2140\u001b[0m \u001b[39mif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misreg():\n\u001b[0;32m-> 2141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmakefile(tarinfo, targetpath)\n\u001b[1;32m   2142\u001b[0m \u001b[39melif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misdir():\n\u001b[1;32m   2143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedir(tarinfo, targetpath)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/tarfile.py:2190\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[0;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[1;32m   2188\u001b[0m     target\u001b[39m.\u001b[39mtruncate()\n\u001b[1;32m   2189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2190\u001b[0m     copyfileobj(source, target, tarinfo\u001b[39m.\u001b[39;49msize, ReadError, bufsize)\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/tarfile.py:250\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buf) \u001b[39m<\u001b[39m bufsize:\n\u001b[1;32m    249\u001b[0m         \u001b[39mraise\u001b[39;00m exception(\u001b[39m\"\u001b[39m\u001b[39munexpected end of data\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m     dst\u001b[39m.\u001b[39;49mwrite(buf)\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m remainder \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    253\u001b[0m     buf \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mread(remainder)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "places_tdatasets.Places365(root='/home/ubuntu/data/Places365_small', split='train-standard',small=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36500"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_test = datasets.Places365(root='/home/ubuntu/data/Places365_small', split='val',small=True, download=False, transform=test_transform)\n",
    "len(places_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1141it [00:41, 27.32it/s]                          \n",
      "2282it [01:30, 25.20it/s]                          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((36500, 512), (36500, 1000))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_feats_test = get_features(model, places_test)\n",
    "places_logits_test  = get_logits(places_feats_test , model.fc.cpu())\n",
    "places_feats_test.shape, places_logits_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2282it [01:30, 25.14it/s]                          \n",
      "2282it [01:29, 25.49it/s]                          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9647945205479452"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_places_feats_test = do_feat_recon(places_logits_test, recon_model)\n",
    "places_preds_orig = get_logits(places_feats_test, model.fc.cpu())\n",
    "places_preds_recon = get_logits(recon_places_feats_test,  model.fc.cpu())\n",
    "top_recon = np.argmax(places_preds_recon, axis=1)\n",
    "top_orig = np.argmax(places_preds_orig, axis=1)\n",
    "np.mean(top_orig == top_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031231464818120003"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.MSELoss()(torch.FloatTensor(recon_places_feats_test), \n",
    "                torch.FloatTensor(places_feats_test)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content_understanding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d36b139402f0f8909133622e5e80cdd43397350f551386f6df555aa508ab69d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
